{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IDS703FinalProject.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxjt5j1rWqwH"
      },
      "source": [
        "# IDS703 Final Project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VgBhHtJR7h_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f76a5add-e2a3-4fdc-b209-cdbbe291d5d6"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import operator\n",
        "from operator import itemgetter\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from nltk.corpus import brown\n",
        "import warnings\n",
        "import random\n",
        "\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import treebank\n",
        "nltk.download('treebank')\n",
        "from nltk.corpus import conll2000\n",
        "nltk.download('conll2000')\n",
        "nltk.download('universal_tagset')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJOrtAA5RreA"
      },
      "source": [
        "## Step 2: Generative Probability Base Model\n",
        "1. Model explanation\n",
        "2. Accuracy explanation\n",
        "\n",
        "Sklearn classification metrics report\n",
        "sklearn library F1, Recall, Precision\n",
        "Sklearn ROC curves\n",
        "\n",
        "qualitatively: edge cases\n",
        "quantitatively: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQGOzvogp6ke"
      },
      "source": [
        "#import pre-marked Part of speech tagging text data from the nltk library\n",
        "treebank = treebank.tagged_sents(tagset='universal')\n",
        "brown = brown.tagged_sents(tagset='universal')\n",
        "conll = conll2000.tagged_sents(tagset='universal')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wom6M5LeE1kQ"
      },
      "source": [
        "# Generative Probabilistic Model: Hidden Markov Model\n",
        "\n",
        "def transition(data):\n",
        "  \"\"\"\n",
        "  Generate a transition matrix from corpus\n",
        "  Sample output: [[0, 0.1, ...], \n",
        "                  [0.2, 0, ...], \n",
        "                  [0, 0]...]]\n",
        "  Each row represents the current tag, \n",
        "  each column represents a tag that followed the current tag\n",
        "  \"\"\"\n",
        "  transition_matrix = []\n",
        "  tags = {}\n",
        "  next_tags = {}\n",
        "\n",
        "  for s in data:\n",
        "    # modify sentence by add a start tag and a end tag (<start>, <end>)\n",
        "    sentence = s.copy()\n",
        "    #sentence.insert(0, ('', '<start>'))\n",
        "    #sentence.append(('', '<end>'))\n",
        "    for i in range(len(sentence) - 1):\n",
        "      tag = sentence[i][1]\n",
        "      next_tag = sentence[i + 1][1]\n",
        "\n",
        "      row_index = None # transition matrix index of current tag\n",
        "      column_index = None # transition matrix index of next tag\n",
        "      # check if current tag exist in the tags dictionary\n",
        "      if tag in tags.keys():\n",
        "        # check next tag exist in the next_tags dictionary\n",
        "        row_index = tags[tag]\n",
        "      else:\n",
        "        row_index = len(tags.keys())\n",
        "        tags[tag] = row_index\n",
        "        if len(transition_matrix) == 0:\n",
        "          transition_matrix.append([])\n",
        "        else:\n",
        "          new_row = [0]*len(transition_matrix[0])\n",
        "          transition_matrix.append(new_row)\n",
        "\n",
        "      # check if word recorded\n",
        "      if next_tag in next_tags.keys():\n",
        "        column_index = next_tags[next_tag]\n",
        "        transition_matrix[row_index][column_index] += 1\n",
        "      else:\n",
        "        column_index = len(next_tags.keys())\n",
        "        next_tags[next_tag] = column_index\n",
        "        for row in transition_matrix:\n",
        "          row.append(0)\n",
        "        transition_matrix[row_index][column_index] = 1\n",
        "\n",
        "  transition_matrix = np.array(transition_matrix) + 1 # added smoothing\n",
        "  transition_matrix = transition_matrix/transition_matrix.sum(axis = 1, keepdims = True)\n",
        "  return transition_matrix\n",
        "\n",
        "def emission(data):\n",
        "  \"\"\"\n",
        "  Generate a observation likelihood matrix (aka emission matrix) from corpus\n",
        "  Sample output: [[0, 0.1, ...], \n",
        "                  [0.2, 0, ...], \n",
        "                  [0, 0]...]]\n",
        "  Each row represents the current tag, \n",
        "  Each column represents a word that's associated with the current tag\n",
        "  \"\"\"\n",
        "  # create a n x m matrix, n = number of unique words, m = number of unique tags\n",
        "  emission_matrix = []\n",
        "  word_index = {} # record word to column index\n",
        "  tag_index = {} # record tag to row index\n",
        "\n",
        "  for sentence in data:\n",
        "    for pair in sentence:\n",
        "      word = pair[0]\n",
        "      tag = pair[1]\n",
        "      row_index = None # tag index in matrix\n",
        "      column_index = None # word index in matrix\n",
        "\n",
        "      # check if tag recorded\n",
        "      row_index = None\n",
        "      column_index = None\n",
        "\n",
        "      # check if tag recorded\n",
        "      if tag in tag_index.keys():\n",
        "        row_index = tag_index[tag]\n",
        "      # tag not exist, add new tag row\n",
        "      else:\n",
        "        row_index = len(tag_index.keys())\n",
        "        tag_index[tag] = row_index\n",
        "        if len(emission_matrix) == 0:\n",
        "          emission_matrix.append([])\n",
        "        else:\n",
        "          new_row = [0]*len(emission_matrix[0])\n",
        "          emission_matrix.append(new_row)\n",
        "      \n",
        "      # check if word recorded\n",
        "      if word in word_index.keys():\n",
        "        column_index = word_index[word]\n",
        "        emission_matrix[row_index][column_index] += 1\n",
        "      else:\n",
        "        column_index = len(word_index.keys())\n",
        "        word_index[word] = column_index\n",
        "        for row in emission_matrix:\n",
        "          row.append(0)\n",
        "        emission_matrix[row_index][column_index] = 1\n",
        "  emission_matrix = np.array(emission_matrix) + 1 # added smoothing\n",
        "  emission_matrix = emission_matrix/emission_matrix.sum(axis = 1, keepdims = True)\n",
        "  return emission_matrix, word_index, tag_index\n",
        "\n",
        "def initial_state(data, tag_index):\n",
        "  initial_state_dist = [0]*len(tag_index.keys())\n",
        "  tags = []\n",
        "  counter = 0\n",
        "  for sentence in data:\n",
        "    counter += 1\n",
        "    tag = sentence[0][1]\n",
        "    index = tag_index[tag]\n",
        "    initial_state_dist[index] += 1\n",
        "  \n",
        "  initial_state_dist = np.array(initial_state_dist) + 1\n",
        "  initial_state_dist = initial_state_dist/initial_state_dist.sum()\n",
        "  return initial_state_dist\n",
        "\n",
        "#OOV\n",
        "def OOV(t1, t3, data, emission_matrix, tag_index):\n",
        "  # compute OOV matrix\n",
        "  tags_pairs = {}\n",
        "  \n",
        "  # populate matrix:\n",
        "  # (tag1, tage2, tag3) ==> {(tag1, tag3): {verb: 1, noun: 3, ...}}\n",
        "  for sentence in data:\n",
        "    for i in range(len(sentence) - 2):\n",
        "      tag1 = sentence[i][1]\n",
        "      tag2 = sentence[i + 1][1]\n",
        "      tag3 = sentence[i + 2][1]\n",
        "      pair = (tag1, tag3)\n",
        "      if pair in tags_pairs.keys():\n",
        "        if tag2 in tags_pairs[pair].keys():\n",
        "          tags_pairs[pair][tag2] += 1\n",
        "        else:\n",
        "          tags_pairs[pair][tag2] = 1\n",
        "      else:\n",
        "          tags_pairs[pair] = {}\n",
        "          tags_pairs[pair][tag2] = 1\n",
        "\n",
        "  # compute distribution\n",
        "  for key in list(tags_pairs.keys()):\n",
        "    pair_list = tags_pairs[key]\n",
        "    total = sum(pair_list.values())\n",
        "    for key2 in list(pair_list.keys()):\n",
        "      tags_pairs[key][key2] = tags_pairs[key][key2]/total\n",
        "\n",
        "  # get appropriate substitute\n",
        "  pair = (t1, t3)\n",
        "  tags = tags_pairs[pair]\n",
        "  max_tag = max(zip(tags.values(), tags.keys()))[1]\n",
        "  max_tag_index = tag_index[max_tag]\n",
        "  max_tag_words = emission_matrix[max_tag_index]\n",
        "  max_word_index = np.argmax(max_tag_words)\n",
        "  #return max_word_index\n",
        "  return max_word_index\n",
        "\n",
        "# part of speech\n",
        "def POS(data):\n",
        "  \"\"\"\n",
        "  This function generates the curcial components of a part of speech tagging HMM\n",
        "  1. transition matrix\n",
        "  2. emission matrix/observation likelihood matrix\n",
        "  3. initial state distribution \n",
        "  \"\"\"\n",
        "\n",
        "  # transition matrix: record the probability of a tag followed by a tag\n",
        "  # procedure: get a tag, count the differenct tags that comes after\n",
        "  transition_df = transition(data)\n",
        "\n",
        "  # observation likelihood matrix: record the probability of a tag associated to a word\n",
        "  emission_matrix, word_index, tag_index = emission(data)\n",
        "\n",
        "  # initial state distribution\n",
        "  initial_state_distribution = initial_state(data, tag_index)\n",
        "\n",
        "  return transition_df, emission_matrix, initial_state_distribution, word_index, tag_index"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwDospKDxwjL"
      },
      "source": [
        "# Viterbi Algorithm (Decoder of the Hidden Markov Model)\n",
        "def viterbi(obs, pi, A, B):\n",
        "  '''\n",
        "  @source: https://en.wikipedia.org/wiki/Viterbi_algorithm pseudocode from Wikipedia\n",
        "  '''\n",
        "  # obs - the observations, list of ints\n",
        "  # pi - the initial state probabilities, list of floats\n",
        "  # A - the state transition probability matrix [2D numpy array]\n",
        "  # B - the observation probability aka emission matrix [2D numpy array]\n",
        "  # states - list of ints\n",
        "  \n",
        "  states = np.zeros(len(obs))\n",
        "  pi = np.log(pi + 1)\n",
        "  A = np.log(A+1)\n",
        "  B = np.log(B+1)\n",
        "  T1 = np.zeros((A.shape[0], len(obs)))\n",
        "  T2 = np.zeros((A.shape[0], len(obs)))\n",
        "  T1[:, 0] = pi * B[:, obs[0]]\n",
        "  T2[:, 0] = 0\n",
        "  for j in range(1, len(obs)):\n",
        "    for k in range(0, A.shape[0]):\n",
        "      T1[k,j] = np.max([T1[i, j - 1] * A[i, k] * B[k, obs[j]] for i in range(0, A.shape[0])])\n",
        "      T2[k,j] = np.argmax([T1[i, j - 1] * A[i, k] * B[k, obs[j]] for i in range(0, A.shape[0])])\n",
        "  states[-1] = np.argmax(T1[:, len(obs) - 1])\n",
        "  for m in range(-1, -(len(obs)), -1):\n",
        "    states[m - 1] = T2[int(states[m]), m]\n",
        "  return states\n",
        "\n",
        "def words_to_index(sentence, word_index, tag_index, emission_matrix, corpus):\n",
        "  obs = []\n",
        "  for i in range(len(sentence)):\n",
        "    pair = sentence[i]\n",
        "    word = pair[0]\n",
        "    if word in word_index.keys():\n",
        "      index = word_index[word]\n",
        "      obs.append(index)\n",
        "    else:\n",
        "      #OOV\n",
        "      pair1 = sentence[i - 1][1]\n",
        "      pair2 = sentence[i][1]\n",
        "      pair3 = sentence[i + 1][1]\n",
        "      index = OOV(pair1, pair3, corpus, emission_matrix, tag_index)\n",
        "      obs.append(index)\n",
        "  return obs "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtHoSNsvR2E5"
      },
      "source": [
        "## Step 3: Neural Network Model\n",
        "\n",
        "See the other Jupyter Notebook for Neural Network Code."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Text Generator Using Viterbi"
      ],
      "metadata": {
        "id": "l5zURj9_2meJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown\n",
        "import pickle\n",
        "nltk.download('brown')\n",
        "data = brown.tagged_sents(tagset='universal')"
      ],
      "metadata": {
        "id": "7i-2AoETf1hn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2b40623-481e-43fd-b6b1-fd95a4934416"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generative Probabilistic Model: Hidden Markov Model\n",
        "def transition_gen(data):\n",
        "  \"\"\"\n",
        "  Generate a transition matrix from corpus\n",
        "  Sample output: [[0, 0.1, ...], \n",
        "                  [0.2, 0, ...], \n",
        "                  [0, 0]...]]\n",
        "  Each row represents the current tag, \n",
        "  each column represents a tag that followed the current tag\n",
        "  \"\"\"\n",
        "  transition_matrix = []\n",
        "  tags = {}\n",
        "  next_tags = {}\n",
        "\n",
        "  for s in data:\n",
        "    # modify sentence by add a start tag and a end tag (<start>, <end>)\n",
        "    sentence = s.copy()\n",
        "    #sentence.insert(0, ('', '<start>'))\n",
        "    #sentence.append(('', '<end>'))\n",
        "    for i in range(len(sentence) - 1):\n",
        "      tag = sentence[i][1]\n",
        "      next_tag = sentence[i + 1][1]\n",
        "\n",
        "      row_index = None # transition matrix index of current tag\n",
        "      column_index = None # transition matrix index of next tag\n",
        "      # check if current tag exist in the tags dictionary\n",
        "      if tag in tags.keys():\n",
        "        # check next tag exist in the next_tags dictionary\n",
        "        row_index = tags[tag]\n",
        "      else:\n",
        "        row_index = len(tags.keys())\n",
        "        tags[tag] = row_index\n",
        "        if len(transition_matrix) == 0:\n",
        "          transition_matrix.append([])\n",
        "        else:\n",
        "          new_row = [0]*len(transition_matrix[0])\n",
        "          transition_matrix.append(new_row)\n",
        "\n",
        "      # check if word recorded\n",
        "      if next_tag in next_tags.keys():\n",
        "        column_index = next_tags[next_tag]\n",
        "        transition_matrix[row_index][column_index] += 1\n",
        "      else:\n",
        "        column_index = len(next_tags.keys())\n",
        "        next_tags[next_tag] = column_index\n",
        "        for row in transition_matrix:\n",
        "          row.append(0)\n",
        "        transition_matrix[row_index][column_index] = 1\n",
        "\n",
        "  transition_matrix = np.array(transition_matrix) + 0.1 # added smoothing\n",
        "  transition_matrix = transition_matrix/transition_matrix.sum(axis = 1, keepdims = True)\n",
        "  return transition_matrix, tags, next_tags\n",
        "\n",
        "def emission_gen(data):\n",
        "  \"\"\"\n",
        "  Generate a observation likelihood matrix (aka emission matrix) from corpus\n",
        "  Sample output: [[0, 0.1, ...], \n",
        "                  [0.2, 0, ...], \n",
        "                  [0, 0]...]]\n",
        "  Each row represents the current tag, \n",
        "  Each column represents a word that's associated with the current tag\n",
        "  \"\"\"\n",
        "  # create a n x m matrix, n = number of unique words, m = number of unique tags\n",
        "  emission_matrix = []\n",
        "  word_index = {} # record word to column index\n",
        "  tag_index = {} # record tag to row index\n",
        "\n",
        "  for sentence in data:\n",
        "    for pair in sentence:\n",
        "      word = pair[0]\n",
        "      tag = pair[1]\n",
        "      row_index = None # tag index in matrix\n",
        "      column_index = None # word index in matrix\n",
        "\n",
        "      # check if tag recorded\n",
        "      row_index = None\n",
        "      column_index = None\n",
        "\n",
        "      # check if tag recorded\n",
        "      if tag in tag_index.keys():\n",
        "        row_index = tag_index[tag]\n",
        "      # tag not exist, add new tag row\n",
        "      else:\n",
        "        row_index = len(tag_index.keys())\n",
        "        tag_index[tag] = row_index\n",
        "        if len(emission_matrix) == 0:\n",
        "          emission_matrix.append([])\n",
        "        else:\n",
        "          new_row = [0]*len(emission_matrix[0])\n",
        "          emission_matrix.append(new_row)\n",
        "      \n",
        "      # check if word recorded\n",
        "      if word in word_index.keys():\n",
        "        column_index = word_index[word]\n",
        "        emission_matrix[row_index][column_index] += 1\n",
        "      else:\n",
        "        column_index = len(word_index.keys())\n",
        "        word_index[word] = column_index\n",
        "        for row in emission_matrix:\n",
        "          row.append(0)\n",
        "        emission_matrix[row_index][column_index] = 1\n",
        "  emission_matrix = np.array(emission_matrix) + 0.1 # added smoothing\n",
        "  emission_matrix = emission_matrix/emission_matrix.sum(axis = 1, keepdims = True)\n",
        "  return emission_matrix, word_index, tag_index\n",
        "\n",
        "def initial_state_gen(data, tag_index):\n",
        "  initial_state_dist = [0]*len(tag_index.keys())\n",
        "  tags = []\n",
        "  counter = 0\n",
        "  for sentence in data:\n",
        "    counter += 1\n",
        "    tag = sentence[0][1]\n",
        "    index = tag_index[tag]\n",
        "    initial_state_dist[index] += 1\n",
        "  \n",
        "  initial_state_dist = np.array(initial_state_dist) + 0.1 # added smoothing\n",
        "  initial_state_dist = initial_state_dist/initial_state_dist.sum()\n",
        "  return initial_state_dist"
      ],
      "metadata": {
        "id": "OXi1HkgA2lfE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tags_generator(prob_df, pi, tag_index, num = 10):\n",
        "  \"\"\"\n",
        "  prob: transition matrix\n",
        "  num: number of tokens to generate\n",
        "  \"\"\"\n",
        "  tokens = []\n",
        "  p = prob_df.to_numpy()\n",
        "  results = [pi]\n",
        "  for i in range(num-1):\n",
        "    results.append(pi@p)\n",
        "    p = p@prob_df.to_numpy()\n",
        "    \n",
        "  for r in results:\n",
        "    tokens.append(np.random.choice(tag_index, p = r))\n",
        "  return tokens\n",
        "\n",
        "def words_generator(prob_df, tag_lst):\n",
        "\n",
        "  words = []\n",
        "  word_bank = list(prob_df.columns)\n",
        "  for tag in tag_lst:\n",
        "    words_dist = prob_df.loc[tag].values\n",
        "    word = np.random.choice(word_bank, p = words_dist)\n",
        "    words.append(word)\n",
        "  return words\n",
        "\n",
        "\n",
        "def create_synthetic_data(tag_sequences, word_sequences):\n",
        "  data = []\n",
        "  for i in range(len(tag_sequences)):\n",
        "    sentence = []\n",
        "    for j in range(len(tag_sequences[i])):\n",
        "      token = (word_sequences[i][j], tag_sequences[i][j])\n",
        "      sentence.append(token)\n",
        "    data.append(sentence)\n",
        "  return data"
      ],
      "metadata": {
        "id": "5dZ5jHHPHCLi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract transition matrix from Viterbi Algorithm \n",
        "transition_matrix, tags, next_tags = transition_gen(data) # generate transition matrix\n",
        "sorted_tags = sorted(tags.items(), key=operator.itemgetter(1))\n",
        "sorted_tags = [x[0] for x in sorted_tags]\n",
        "sorted_next_tags = sorted(next_tags.items(), key=operator.itemgetter(1))\n",
        "sorted_next_tags = [x[0] for x in sorted_next_tags]\n",
        "\n",
        "transition_df = pd.DataFrame(transition_matrix)\n",
        "transition_df.columns = list(sorted_next_tags)\n",
        "transition_df.index = list(sorted_tags)"
      ],
      "metadata": {
        "id": "5R0J-eLDE_vi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emission_matrix, word_index, tag_index = emission_gen(data) # likelihood of word associating with a tag \n",
        "sorted_tag_index = sorted(tag_index.items(), key=operator.itemgetter(1))\n",
        "sorted_tag_index = [x[0] for x in sorted_tag_index]\n",
        "\n",
        "emission_df = pd.DataFrame(emission_matrix)\n",
        "emission_df.columns = list(word_index.keys())\n",
        "emission_df.index = list(sorted_tag_index)"
      ],
      "metadata": {
        "id": "FZ8B6CMfHOcD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pi = initial_state_gen(data, tag_index)"
      ],
      "metadata": {
        "id": "MHeh2enjJ0_r"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate synthetic data in 2D array\n",
        "# generate tags\n",
        "tag_sequences = []\n",
        "for i in range(100):\n",
        "  sequence = tags_generator(transition_df, pi, sorted_tag_index)\n",
        "  if sequence[0] == \".\":\n",
        "    continue\n",
        "  else:\n",
        "    tag_sequences.append(sequence)\n",
        "\n",
        "# generate words\n",
        "word_sequences = []\n",
        "for tags in tag_sequences:\n",
        "  words = words_generator(emission_df, tags)\n",
        "  word_sequences.append(words)\n",
        "\n",
        "# combine tags and words into appropriate input format\n",
        "data = create_synthetic_data(tag_sequences, word_sequences)"
      ],
      "metadata": {
        "id": "Atp9ZJ9rKFP4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save generate synthetic data into pickle file for potential future use\n",
        "with open('test.txt', 'rb') as f:\n",
        "    syn_data = pickle.load(f)"
      ],
      "metadata": {
        "id": "zabLahMpmNMl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import treebank\n",
        "nltk.download('treebank')\n",
        "\n",
        "treebank = treebank.tagged_sents(tagset='universal')\n",
        "brown = brown.tagged_sents(tagset='universal')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4qEjTfB70vC",
        "outputId": "45f80983-77aa-459c-9194-a8cf093762dc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test step 2\n",
        "# train model on syn_data\n",
        "A, B, pi, word_index, tag_index = POS(brown)\n",
        "\n",
        "# test model on syn_data\n",
        "states_data = []\n",
        "for test in syn_data:\n",
        "  obs = words_to_index(test, word_index, tag_index, B, brown)\n",
        "  states = viterbi(obs, pi, A, B)\n",
        "  states_data.append(states)"
      ],
      "metadata": {
        "id": "RLt157lek6dn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics of POS tagging\n",
        "def accuracy(states, tag_index, test):\n",
        "  test_states = []\n",
        "  tag_states = []\n",
        "  for state in states:\n",
        "    for index in state:\n",
        "      key = [k for k, v in tag_index.items() if v == index][0]\n",
        "      tag_states.append(key)\n",
        "  for seq in test:\n",
        "    for t in seq:\n",
        "      key = t[1]\n",
        "      test_states.append(key)\n",
        "\n",
        "  print(classification_report(test_states, tag_states, digits=3))"
      ],
      "metadata": {
        "id": "nyMPwJTPk7TQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy(states_data, tag_index, syn_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "in9vCxe6wjkG",
        "outputId": "d09a14a8-10a9-4b8f-ea9e-6af1ac9a73ac"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           .      0.940     0.988     0.963        80\n",
            "         ADJ      0.792     0.805     0.798       118\n",
            "         ADP      0.763     0.879     0.817        66\n",
            "         ADV      0.850     0.654     0.739        52\n",
            "        CONJ      0.912     0.775     0.838        40\n",
            "         DET      0.867     0.964     0.913       358\n",
            "        NOUN      0.813     0.763     0.787       114\n",
            "         NUM      0.917     0.550     0.687        20\n",
            "        PRON      0.800     0.900     0.847        40\n",
            "         PRT      0.667     0.421     0.516        19\n",
            "        VERB      0.844     0.529     0.651        51\n",
            "           X      0.000     0.000     0.000         2\n",
            "\n",
            "    accuracy                          0.845       960\n",
            "   macro avg      0.764     0.686     0.713       960\n",
            "weighted avg      0.842     0.845     0.838       960\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Testing With Real Data"
      ],
      "metadata": {
        "id": "BagCekbr9h-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_index = np.random.randint(0, len(brown), 500)\n",
        "train_index = list(set(list(range(0, len(brown)))) - set(test_index))"
      ],
      "metadata": {
        "id": "BSchZLo7-Cez"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = itemgetter(*train_index)(brown)\n",
        "test_data = itemgetter(*test_index)(brown)"
      ],
      "metadata": {
        "id": "jOhdEXRa-pgw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test step 2\n",
        "# train model on syn_data\n",
        "A, B, pi, word_index, tag_index = POS(train_data)\n",
        "\n",
        "# test model on syn_data\n",
        "states_data = []\n",
        "for test in test_data:\n",
        "  obs = words_to_index(test, word_index, tag_index, B, train_data)\n",
        "  states = viterbi(obs, pi, A, B)\n",
        "  states_data.append(states)"
      ],
      "metadata": {
        "id": "qj76o_U09k_H"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics of POS tagging\n",
        "def accuracy(states, tag_index, test):\n",
        "  test_states = []\n",
        "  tag_states = []\n",
        "  for state in states:\n",
        "    for index in state:\n",
        "      key = [k for k, v in tag_index.items() if v == index][0]\n",
        "      tag_states.append(key)\n",
        "  for seq in test:\n",
        "    for t in seq:\n",
        "      tag = t[1]\n",
        "      test_states.append(tag)\n",
        "  print(classification_report(test_states, tag_states, digits=3))"
      ],
      "metadata": {
        "id": "yR6X52dpE3k2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy(states_data, tag_index, test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aquwpYiZARCS",
        "outputId": "88f815b6-0fa2-4dfd-d35b-016f5407fc80"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           .      0.928     1.000     0.963      1250\n",
            "         ADJ      0.791     0.773     0.782       768\n",
            "         ADP      0.785     0.968     0.867      1258\n",
            "         ADV      0.882     0.826     0.853       535\n",
            "        CONJ      0.987     1.000     0.993       296\n",
            "         DET      0.751     0.976     0.849      1186\n",
            "        NOUN      0.961     0.774     0.857      2424\n",
            "         NUM      0.950     0.800     0.869       120\n",
            "        PRON      0.901     0.958     0.928       400\n",
            "         PRT      0.781     0.433     0.557       247\n",
            "        VERB      0.933     0.862     0.896      1549\n",
            "           X      0.917     0.611     0.733        18\n",
            "\n",
            "    accuracy                          0.872     10051\n",
            "   macro avg      0.881     0.832     0.846     10051\n",
            "weighted avg      0.882     0.872     0.870     10051\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rT54yNX9AZwd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
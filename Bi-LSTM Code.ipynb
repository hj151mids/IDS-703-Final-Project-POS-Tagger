{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cfc3df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3bef75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f77df30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0+cu102\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a330d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69c27097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1050'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64065f3",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ff98313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function, grab data from torchtext\n",
    "#Field helps us handle how the data will be processed and sent to us.\n",
    "#In this case, we set lower=True to get all words in lower case\n",
    "#We will train and test our model using the UDPOS, which stands for universal dependency pos tagging\n",
    "#This torchtext dataset is pre-labelled with UD pos tags\n",
    "#Load the UDPOS dataset and then do the train test splitting\n",
    "def get_UDPOS():\n",
    "    Fields = ((\"text\", torchtext.legacy.data.Field(lower=True)), \n",
    "              (\"udtags\", torchtext.legacy.data.Field(unk_token=None)), (None, None))\n",
    "    train, val, test = torchtext.legacy.datasets.UDPOS.splits(Fields)\n",
    "    return (train, val, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8d18dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepData():\n",
    "    def __init__(self):\n",
    "        self.text = torchtext.legacy.data.Field(lower=True)\n",
    "        self.tags = torchtext.legacy.data.Field(unk_token=None)\n",
    "        self.Fields = ((\"text\", self.text), (\"udtags\", self.tags), (None, None))\n",
    "        self.train = self.get_UDPOS()[0]\n",
    "        self.val = self.get_UDPOS()[1]\n",
    "        self.test = self.get_UDPOS()[2]\n",
    "        self.trainIter = self.iterator()[0]\n",
    "        self.valIter = self.iterator()[1]\n",
    "        self.testIter = self.iterator()[2]\n",
    "    def get_UDPOS(self):\n",
    "        Train, Val, Test = torchtext.legacy.datasets.UDPOS.splits(self.Fields)\n",
    "        return (Train, Val, Test)\n",
    "    def vectorize_text(self):\n",
    "        #Use pre-trained word vector from GloVe, \n",
    "        #Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download\n",
    "        self.text.build_vocab(self.train, min_freq = 2, vectors = \"glove.6B.50d\")\n",
    "    def vectorize_tags(self):\n",
    "        self.tags.build_vocab(self.train)\n",
    "    #BucketIterator can batch sentences of similar lengths together to minimize\n",
    "    #the amount of padding needed\n",
    "    #This function will define such bucket iterator to load batches of data from the UDP dataset\n",
    "    def iterator(self):\n",
    "        TrainIt, ValIt, TestIt = torchtext.legacy.data.BucketIterator.splits(\n",
    "        (self.train, self.val, self.test),\n",
    "        batch_size = 100,\n",
    "        device = torch.device(\"cuda\"))\n",
    "        return (TrainIt, ValIt, TestIt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88807bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize and prepare data\n",
    "prepdata = PrepData()\n",
    "text = prepdata.text\n",
    "tags = prepdata.tags\n",
    "Fields = prepdata.Fields\n",
    "train, val, test = prepdata.train, prepdata.val, prepdata.test\n",
    "trainIter, valIter, testIter = prepdata.trainIter, prepdata.valIter, prepdata.testIter\n",
    "#prepdata.vectorize_text()\n",
    "#prepdata.vectorize_tags()\n",
    "text.build_vocab(train, min_freq = 2, vectors = \"glove.6B.50d\")\n",
    "tags.build_vocab(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca42596d",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53200fdf",
   "metadata": {},
   "source": [
    "#### The design of our bi-lstm model looks like the plot below. The tokens will first enter an embedding layer for encoding. And then they will be fed to a bidirectional LSTM layer with pretrained weights. Last but not least, we can get predicted tags after going through the linear layer. Dropout layers are added to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94bb773d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"PytorchBiLSTM.jpg\" width=\"600\" height=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url= \"PytorchBiLSTM.jpg\", width=600, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c28007bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the structure of the POSTagging neural network. \n",
    "# This class first initializes the variables and then the forward function implements the structure\n",
    "\n",
    "class POSTagging(torch.nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, hidden_size, dropout, output_size):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx = 1)\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_size, num_layers = 2, dropout=dropout, bidirectional=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size*2, output_size)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "    def forward(self, text):\n",
    "        embedded_text = self.dropout(self.embedding(text))\n",
    "        lstm_out, (hidden_state, cell_state) = self.lstm(embedded_text)\n",
    "        return self.linear(self.dropout(lstm_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb454907",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the network\n",
    "net = POSTagging(len(text.vocab), 50, 64, 0.2, len(tags.vocab))\n",
    "#load pretrained embedding values into the embedding layer\n",
    "net.embedding.weight.data.copy_(text.vocab.vectors)\n",
    "net.embedding.weight.data[1] = torch.zeros(50)\n",
    "#define the loss to be the cross entropy loss, which works well with multi-class classification problem\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index = 0)\n",
    "#throw the jobs to nvidia card\n",
    "net = net.to('cuda')\n",
    "criterion = criterion.to('cuda')\n",
    "#We use adam for gradient descent to optimize the parameters\n",
    "optimizer = torch.optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "485bff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main function helps with training, validation, and testing processes.\n",
    "# Each chunck of conditional loop is responsible for reading in the data (in iterator format), \n",
    "# making predictions, as well as keeping track of loss and accuracy\n",
    "def main(validation, evaluation=False):\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    if validation == False and evaluation == False:\n",
    "        net.train()\n",
    "        for i in trainIter:\n",
    "            X = i.text\n",
    "            y = i.udtags\n",
    "            optimizer.zero_grad()\n",
    "            pred = net(X)\n",
    "            pred = pred.view(-1, pred.shape[-1])\n",
    "            y = y.view(-1)\n",
    "            loss = criterion(pred,y)\n",
    "            #when calculating accuracy, one should not discard unknown tokens\n",
    "            accuracy = pred.argmax(dim = 1, keepdim = True)[(y != 0).nonzero()].squeeze(1).eq(y[(y != 0).nonzero()]).sum() / y[(y != 0).nonzero()].shape[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            total_accuracy += accuracy.item()\n",
    "        training_loss = total_loss / len(trainIter)\n",
    "        training_accuracy = total_accuracy / len(trainIter)\n",
    "        return training_loss, training_accuracy\n",
    "    elif validation == True and evaluation == False:\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in valIter:\n",
    "                X = i.text\n",
    "                y = i.udtags\n",
    "                pred = net(X)\n",
    "                pred = pred.view(-1, pred.shape[-1])\n",
    "                y = y.view(-1)\n",
    "                loss = criterion(pred, y)\n",
    "                accuracy = pred.argmax(dim = 1, keepdim = True)[(y != 0).nonzero()].squeeze(1).eq(y[(y != 0).nonzero()]).sum() / y[(y != 0).nonzero()].shape[0]\n",
    "                total_loss += loss.item()\n",
    "                total_accuracy += accuracy.item()\n",
    "            validation_loss = total_loss / len(valIter)\n",
    "            validation_accuracy = total_accuracy / len(valIter)\n",
    "            return validation_loss, validation_accuracy\n",
    "    else:\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in testIter:\n",
    "                X = i.text\n",
    "                y = i.udtags\n",
    "                pred = net(X)\n",
    "                pred = pred.view(-1, pred.shape[-1])\n",
    "                y = y.view(-1)\n",
    "                loss = criterion(pred, y)\n",
    "                accuracy = pred.argmax(dim = 1, keepdim = True)[(y != 0).nonzero()].squeeze(1).eq(y[(y != 0).nonzero()]).sum() / y[(y != 0).nonzero()].shape[0]\n",
    "                total_loss += loss.item()\n",
    "                total_accuracy += accuracy.item()\n",
    "            testing_loss = total_loss / len(testIter)\n",
    "            testing_accuracy = total_accuracy / len(testIter)\n",
    "            return testing_loss, testing_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "899132a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\t Training Loss: 1.933 , Training Accuracy: 0.390\n",
      "\t Validation Loss: 1.167 ,  Validation Accuracy: 0.643\n",
      "\t Testing Loss: 1.189 ,  Testing Accuracy: 0.645\n",
      "Epoch: 2\n",
      "\t Training Loss: 0.748 , Training Accuracy: 0.772\n",
      "\t Validation Loss: 0.710 ,  Validation Accuracy: 0.792\n",
      "\t Testing Loss: 0.720 ,  Testing Accuracy: 0.791\n",
      "Epoch: 3\n",
      "\t Training Loss: 0.485 , Training Accuracy: 0.850\n",
      "\t Validation Loss: 0.597 ,  Validation Accuracy: 0.820\n",
      "\t Testing Loss: 0.596 ,  Testing Accuracy: 0.825\n",
      "Epoch: 4\n",
      "\t Training Loss: 0.391 , Training Accuracy: 0.879\n",
      "\t Validation Loss: 0.546 ,  Validation Accuracy: 0.833\n",
      "\t Testing Loss: 0.540 ,  Testing Accuracy: 0.839\n",
      "Epoch: 5\n",
      "\t Training Loss: 0.334 , Training Accuracy: 0.896\n",
      "\t Validation Loss: 0.500 ,  Validation Accuracy: 0.846\n",
      "\t Testing Loss: 0.490 ,  Testing Accuracy: 0.852\n",
      "Epoch: 6\n",
      "\t Training Loss: 0.298 , Training Accuracy: 0.907\n",
      "\t Validation Loss: 0.479 ,  Validation Accuracy: 0.849\n",
      "\t Testing Loss: 0.467 ,  Testing Accuracy: 0.854\n",
      "Epoch: 7\n",
      "\t Training Loss: 0.269 , Training Accuracy: 0.916\n",
      "\t Validation Loss: 0.453 ,  Validation Accuracy: 0.856\n",
      "\t Testing Loss: 0.443 ,  Testing Accuracy: 0.860\n",
      "Epoch: 8\n",
      "\t Training Loss: 0.249 , Training Accuracy: 0.922\n",
      "\t Validation Loss: 0.440 ,  Validation Accuracy: 0.861\n",
      "\t Testing Loss: 0.427 ,  Testing Accuracy: 0.867\n",
      "Epoch: 9\n",
      "\t Training Loss: 0.230 , Training Accuracy: 0.927\n",
      "\t Validation Loss: 0.433 ,  Validation Accuracy: 0.858\n",
      "\t Testing Loss: 0.417 ,  Testing Accuracy: 0.869\n",
      "Epoch: 10\n",
      "\t Training Loss: 0.217 , Training Accuracy: 0.931\n",
      "\t Validation Loss: 0.431 ,  Validation Accuracy: 0.859\n",
      "\t Testing Loss: 0.418 ,  Testing Accuracy: 0.869\n"
     ]
    }
   ],
   "source": [
    "#Training the model with 10 epochs on GPU\n",
    "#Training accuracy keeps increasing, which is a good sign\n",
    "for i in range(10):\n",
    "    training_loss, training_accuracy = main(False)\n",
    "    validation_loss, validation_accuracy = main(True)\n",
    "    testing_loss, testing_accuracy = main(False, True)\n",
    "    print(f'Epoch: {i + 1:1}')\n",
    "    print(f'\\t Training Loss: {training_loss:.3f} , Training Accuracy: {training_accuracy:.3f}')\n",
    "    print(f'\\t Validation Loss: {validation_loss:.3f} ,  Validation Accuracy: {validation_accuracy:.3f}')\n",
    "    print(f'\\t Testing Loss: {testing_loss:.3f} ,  Testing Accuracy: {testing_accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6687be21",
   "metadata": {},
   "source": [
    "## Testing on Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2288339",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read our synthetic data file generated in another part\n",
    "import pickle\n",
    "with open ('test_data', 'rb') as fp:\n",
    "    test_data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c4839a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('here', 'ADV'),\n",
       "  ('claim', 'NOUN'),\n",
       "  ('the', 'DET'),\n",
       "  ('quest', 'NOUN'),\n",
       "  ('stuff', 'NOUN'),\n",
       "  ('the', 'DET'),\n",
       "  ('word', 'NOUN'),\n",
       "  ('dilute', 'ADJ'),\n",
       "  ('doll', 'NOUN'),\n",
       "  ('sooner', 'ADV')],\n",
       " [('residential', 'ADJ'),\n",
       "  (\"''\", '.'),\n",
       "  ('the', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('some', 'DET'),\n",
       "  ('A', 'DET'),\n",
       "  ('output', 'NOUN'),\n",
       "  ('deeper', 'ADJ'),\n",
       "  ('the', 'DET')],\n",
       " [('him', 'PRON'),\n",
       "  ('mastiff', 'CONJ'),\n",
       "  ('his', 'DET'),\n",
       "  ('thoroughly', 'ADV'),\n",
       "  ('Among', 'ADP'),\n",
       "  ('as', 'ADP'),\n",
       "  ('young', 'ADJ'),\n",
       "  ('An', 'DET'),\n",
       "  ('desperate', 'ADJ'),\n",
       "  ('Judge', 'NOUN')],\n",
       " [('the', 'DET'),\n",
       "  ('by', 'ADP'),\n",
       "  ('his', 'DET'),\n",
       "  ('which', 'DET'),\n",
       "  ('into', 'ADP'),\n",
       "  ('high', 'ADJ'),\n",
       "  ('that', 'PRON'),\n",
       "  ('necessary', 'ADJ'),\n",
       "  ('those', 'DET'),\n",
       "  ('of', 'ADP')],\n",
       " [('in', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('therapeutic', 'ADJ'),\n",
       "  ('a', 'DET'),\n",
       "  ('This', 'DET'),\n",
       "  ('of', 'ADP'),\n",
       "  ('not', 'ADV'),\n",
       "  ('nothing', 'NOUN'),\n",
       "  ('as', 'ADP'),\n",
       "  ('The', 'DET')],\n",
       " [('His', 'DET'),\n",
       "  ('Congregational', 'ADJ'),\n",
       "  ('passivity', 'NUM'),\n",
       "  ('accesses', 'NOUN'),\n",
       "  ('Alexeyeva', 'DET'),\n",
       "  ('a', 'DET'),\n",
       "  ('This', 'DET'),\n",
       "  (',', '.'),\n",
       "  ('order', 'NOUN'),\n",
       "  ('the', 'DET')],\n",
       " [('the', 'DET'),\n",
       "  ('additional', 'ADJ'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('eclipses', 'DET'),\n",
       "  ('a', 'DET'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('terrible', 'ADJ'),\n",
       "  ('a', 'DET'),\n",
       "  ('he', 'PRON'),\n",
       "  ('what', 'DET')],\n",
       " [('hold', 'VERB'),\n",
       "  ('E', 'NOUN'),\n",
       "  ('its', 'DET'),\n",
       "  ('a', 'DET'),\n",
       "  ('deferments', 'DET'),\n",
       "  ('2', 'NUM'),\n",
       "  ('the', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('work', 'NOUN')],\n",
       " [('remark', 'NOUN'),\n",
       "  ('work', 'NOUN'),\n",
       "  ('!', '.'),\n",
       "  ('life', 'NOUN'),\n",
       "  ('a', 'DET'),\n",
       "  ('Even', 'ADV'),\n",
       "  (',', '.'),\n",
       "  ('civil', 'ADJ'),\n",
       "  ('economic', 'ADJ'),\n",
       "  ('soot', 'ADJ')],\n",
       " [('when', 'ADV'),\n",
       "  ('this', 'DET'),\n",
       "  ('This', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('all', 'PRT'),\n",
       "  ('the', 'DET'),\n",
       "  ('or', 'CONJ'),\n",
       "  ('on', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('the', 'DET')],\n",
       " [('had', 'VERB'),\n",
       "  ('.', '.'),\n",
       "  ('a', 'DET'),\n",
       "  ('ideal', 'ADJ'),\n",
       "  ('light', 'ADJ'),\n",
       "  ('--', '.'),\n",
       "  ('so', 'ADV'),\n",
       "  ('the', 'DET'),\n",
       "  ('evening', 'NOUN'),\n",
       "  ('.', '.')],\n",
       " [('was', 'VERB'),\n",
       "  ('made', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('trustingly', 'ADV'),\n",
       "  ('the', 'DET'),\n",
       "  ('Washington', 'NOUN'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('the', 'DET'),\n",
       "  ('modal', 'CONJ'),\n",
       "  ('the', 'DET')],\n",
       " [('a', 'DET'),\n",
       "  ('same', 'ADJ'),\n",
       "  ('to', 'PRT'),\n",
       "  ('The', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('mission', 'NOUN'),\n",
       "  ('the', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('put', 'VERB'),\n",
       "  ('Mellow', 'ADJ')],\n",
       " [('and', 'CONJ'),\n",
       "  ('way', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('active', 'ADJ'),\n",
       "  ('date', 'NOUN'),\n",
       "  ('bearded', 'ADJ'),\n",
       "  ('through', 'ADP'),\n",
       "  ('1594-1674', 'ADV'),\n",
       "  ('some', 'DET'),\n",
       "  ('questionnaire', 'NOUN')],\n",
       " [('and', 'CONJ'),\n",
       "  ('a', 'DET'),\n",
       "  ('bowl', 'NOUN'),\n",
       "  ('lumbering', '.'),\n",
       "  ('national', 'ADJ'),\n",
       "  (',', '.'),\n",
       "  ('undeniable', 'ADJ'),\n",
       "  ('feeding-pain', 'NOUN'),\n",
       "  ('release', 'NOUN'),\n",
       "  ('N.', 'NOUN')],\n",
       " [('good-bye', 'PRT'),\n",
       "  ('more', 'ADJ'),\n",
       "  ('her', 'DET'),\n",
       "  ('a', 'DET'),\n",
       "  ('this', 'DET'),\n",
       "  ('tough', 'ADJ'),\n",
       "  ('wars', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  (',', '.'),\n",
       "  ('.', '.')],\n",
       " [('thought', 'VERB'),\n",
       "  ('a', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('What', 'DET'),\n",
       "  ('wayward', 'ADJ'),\n",
       "  ('new', 'ADJ'),\n",
       "  ('the', 'DET'),\n",
       "  ('these', 'DET'),\n",
       "  ('``', '.')],\n",
       " [('a', 'DET'),\n",
       "  ('along', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('to', 'ADP'),\n",
       "  ('when', 'ADV'),\n",
       "  ('The', 'DET'),\n",
       "  ('minds', 'NOUN'),\n",
       "  ('each', 'DET'),\n",
       "  ('than', 'ADP'),\n",
       "  ('that', 'DET')],\n",
       " [('these', 'DET'),\n",
       "  ('could', 'VERB'),\n",
       "  ('addition', 'NOUN'),\n",
       "  ('usual', 'ADJ'),\n",
       "  ('the', 'DET'),\n",
       "  ('person', 'NOUN'),\n",
       "  ('this', 'DET'),\n",
       "  ('due', 'ADJ'),\n",
       "  ('anger', 'NOUN'),\n",
       "  ('Swiftly', 'ADV')],\n",
       " [('world', 'NOUN'),\n",
       "  ('Langford', 'NOUN'),\n",
       "  ('international', 'ADJ'),\n",
       "  ('both', 'DET'),\n",
       "  ('a', 'DET'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('primates', 'CONJ'),\n",
       "  ('flown', 'VERB'),\n",
       "  ('being', 'NOUN'),\n",
       "  ('prime', 'ADJ')],\n",
       " [('never', 'ADV'),\n",
       "  ('What', 'DET'),\n",
       "  ('many', 'ADJ'),\n",
       "  ('.', '.'),\n",
       "  ('her', 'DET'),\n",
       "  ('road', 'NOUN'),\n",
       "  ('their', 'DET'),\n",
       "  ('And', 'CONJ'),\n",
       "  ('1920', 'NUM'),\n",
       "  ('not', 'ADV')],\n",
       " [('our', 'DET'),\n",
       "  ('one-sixth', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('I', 'PRON'),\n",
       "  ('their', 'DET'),\n",
       "  ('.', '.'),\n",
       "  ('a', 'DET'),\n",
       "  ('our', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  (\"world's\", 'NOUN')],\n",
       " [('where', 'ADV'),\n",
       "  (',', '.'),\n",
       "  ('his', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('away', 'ADV'),\n",
       "  ('chansons', 'ADJ'),\n",
       "  ('that', 'ADP'),\n",
       "  ('wide', 'ADJ'),\n",
       "  ('the', 'DET'),\n",
       "  (',', '.')],\n",
       " [('C', 'NOUN'),\n",
       "  ('dark', 'ADJ'),\n",
       "  ('the', 'DET'),\n",
       "  ('his', 'DET'),\n",
       "  ('stamped', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('just', 'ADV'),\n",
       "  ('that', 'DET'),\n",
       "  ('as', 'ADV'),\n",
       "  ('to', 'PRT')],\n",
       " [('no', 'DET'),\n",
       "  ('what', 'DET'),\n",
       "  ('no', 'ADV'),\n",
       "  ('War', 'NOUN'),\n",
       "  ('something', 'NOUN'),\n",
       "  ('the', 'DET'),\n",
       "  ('unusual', 'ADJ'),\n",
       "  ('the', 'DET'),\n",
       "  ('The', 'DET'),\n",
       "  ('a', 'DET')],\n",
       " [('quintets', 'DET'),\n",
       "  ('vehicles', 'NOUN'),\n",
       "  ('which', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('other', 'ADJ'),\n",
       "  ('the', 'DET'),\n",
       "  (\"I'm\", 'PRT'),\n",
       "  ('out', 'PRT'),\n",
       "  ('both', 'DET'),\n",
       "  ('the', 'DET')],\n",
       " [('you', 'PRON'),\n",
       "  ('Consonantal', 'ADJ'),\n",
       "  ('the', 'DET'),\n",
       "  ('name', 'NOUN'),\n",
       "  ('The', 'DET'),\n",
       "  ('her', 'DET'),\n",
       "  ('aluminum', 'NOUN'),\n",
       "  ('1', 'NUM'),\n",
       "  ('the', 'DET'),\n",
       "  ('hotel', 'NOUN')],\n",
       " [('likes', 'VERB'),\n",
       "  ('law-enforcement', '.'),\n",
       "  ('a', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('.', '.'),\n",
       "  ('cardiomegaly', 'DET'),\n",
       "  ('an', 'DET'),\n",
       "  ('bitterly', 'ADV'),\n",
       "  ('the', 'DET'),\n",
       "  ('an', 'DET')],\n",
       " [('and', 'CONJ'),\n",
       "  (',', '.'),\n",
       "  ('Af', 'NOUN'),\n",
       "  ('corresponding', 'ADJ'),\n",
       "  ('Armed', 'VERB'),\n",
       "  ('some', 'DET'),\n",
       "  ('Just', 'ADV'),\n",
       "  ('the', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('your', 'DET')],\n",
       " [('and', 'CONJ'),\n",
       "  ('stub', 'NOUN'),\n",
       "  ('best', 'ADJ'),\n",
       "  ('instead', 'ADV'),\n",
       "  (',', '.'),\n",
       "  (';', '.'),\n",
       "  ('a', 'DET'),\n",
       "  ('irritable', 'ADJ'),\n",
       "  ('no', 'DET'),\n",
       "  ('.', '.')],\n",
       " [('unblinkingly', 'ADV'),\n",
       "  ('future', 'ADJ'),\n",
       "  ('that', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('Conceding', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('Jail', 'NOUN'),\n",
       "  ('.', '.'),\n",
       "  ('a', 'DET')],\n",
       " [('of', 'ADP'),\n",
       "  ('with', 'ADP'),\n",
       "  ('she', 'PRON'),\n",
       "  ('any', 'DET'),\n",
       "  (',', '.'),\n",
       "  ('corporate', 'ADJ'),\n",
       "  ('sea', 'NOUN'),\n",
       "  ('what', 'DET'),\n",
       "  ('a', 'DET'),\n",
       "  ('was', 'VERB')],\n",
       " [('she', 'PRON'),\n",
       "  ('understand', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('establishing', 'VERB'),\n",
       "  ('or', 'CONJ'),\n",
       "  ('the', 'DET'),\n",
       "  ('his', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('Wisely', 'CONJ'),\n",
       "  ('58th', 'ADP')],\n",
       " [('aid', 'NOUN'),\n",
       "  ('the', 'DET'),\n",
       "  ('into', 'ADP'),\n",
       "  ('no', 'DET'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('his', 'DET'),\n",
       "  ('something', 'NOUN'),\n",
       "  ('Sumter', 'ADP'),\n",
       "  ('New', 'ADJ'),\n",
       "  ('the', 'DET')],\n",
       " [('thoughts', 'NOUN'),\n",
       "  ('that', 'PRON'),\n",
       "  ('a', 'DET'),\n",
       "  ('These', 'DET'),\n",
       "  ('This', 'DET'),\n",
       "  ('.', '.'),\n",
       "  ('lovely', 'ADJ'),\n",
       "  ('Could', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('Grzesiak', 'NUM')],\n",
       " [('he', 'PRON'),\n",
       "  ('gets', 'VERB'),\n",
       "  ('a', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('a', 'DET'),\n",
       "  ('a', 'DET'),\n",
       "  ('with', 'ADP'),\n",
       "  ('day', 'NOUN'),\n",
       "  ('accomplished', 'VERB'),\n",
       "  ('centrally', 'ADV')],\n",
       " [('partly', 'ADV'),\n",
       "  ('him', 'PRON'),\n",
       "  ('in', 'PRT'),\n",
       "  ('reassembled', 'DET'),\n",
       "  ('shaking', 'VERB'),\n",
       "  (',', '.'),\n",
       "  ('the', 'DET'),\n",
       "  ('.', '.'),\n",
       "  (',', '.'),\n",
       "  ('this', 'DET')],\n",
       " [('again', 'ADV'),\n",
       "  ('the', 'DET'),\n",
       "  ('a', 'DET'),\n",
       "  ('.', '.'),\n",
       "  ('active', 'ADJ'),\n",
       "  ('the', 'DET'),\n",
       "  ('for', 'ADP'),\n",
       "  ('defense', 'NOUN'),\n",
       "  ('the', 'DET'),\n",
       "  ('100', 'NUM')],\n",
       " [('he', 'PRON'),\n",
       "  ('in', 'ADP'),\n",
       "  ('stood', 'VERB'),\n",
       "  ('a', 'DET'),\n",
       "  ('dusty', 'ADJ'),\n",
       "  ('slow', 'ADJ'),\n",
       "  ('by', 'ADP'),\n",
       "  ('a', 'DET'),\n",
       "  ('its', 'DET'),\n",
       "  ('we', 'PRON')],\n",
       " [('and', 'CONJ'),\n",
       "  (',', '.'),\n",
       "  ('all', 'PRT'),\n",
       "  ('military', 'ADJ'),\n",
       "  ('railroad', 'NOUN'),\n",
       "  ('by', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('bloodlust', 'NOUN'),\n",
       "  ('right', 'ADV'),\n",
       "  ('the', 'DET')],\n",
       " [('of', 'ADP'),\n",
       "  ('a', 'DET'),\n",
       "  ('this', 'DET'),\n",
       "  ('products', 'NOUN'),\n",
       "  ('1', 'NUM'),\n",
       "  ('inventory', 'NOUN'),\n",
       "  ('dominant', 'DET'),\n",
       "  ('no', 'DET'),\n",
       "  ('.', '.'),\n",
       "  ('The', 'DET')],\n",
       " [('the', 'DET'),\n",
       "  ('as', 'ADP'),\n",
       "  ('disappointed', 'VERB'),\n",
       "  ('The', 'DET'),\n",
       "  ('which', 'DET'),\n",
       "  ('a', 'DET'),\n",
       "  ('improvement', 'NOUN'),\n",
       "  (\"It's\", 'PRT'),\n",
       "  ('of', 'ADP'),\n",
       "  ('when', 'ADV')],\n",
       " [('airframe', 'DET'),\n",
       "  ('answer', 'VERB'),\n",
       "  ('that', 'DET'),\n",
       "  ('It', 'PRON'),\n",
       "  ('?', '.'),\n",
       "  ('the', 'DET'),\n",
       "  ('Bess', 'NUM'),\n",
       "  ('The', 'DET'),\n",
       "  ('any', 'DET'),\n",
       "  ('ethical', 'ADJ')],\n",
       " [('easily', 'ADV'),\n",
       "  ('less', 'ADJ'),\n",
       "  ('the', 'DET'),\n",
       "  ('Group', 'ADP'),\n",
       "  ('carpets', 'NOUN'),\n",
       "  ('large', 'ADJ'),\n",
       "  ('``', '.'),\n",
       "  ('Location', 'NOUN'),\n",
       "  ('20', 'NUM'),\n",
       "  ('god', 'NOUN')],\n",
       " [('In', 'ADP'),\n",
       "  ('does', 'VERB'),\n",
       "  ('apartment', 'NOUN'),\n",
       "  ('with', 'ADP'),\n",
       "  ('a', 'DET'),\n",
       "  ('list', 'NOUN'),\n",
       "  ('a', 'DET'),\n",
       "  ('his', 'DET'),\n",
       "  ('``', '.'),\n",
       "  ('hole', 'NOUN')],\n",
       " [('her', 'PRON'),\n",
       "  ('world-wide', 'ADJ'),\n",
       "  ('.', '.'),\n",
       "  ('the', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('.', '.'),\n",
       "  ('rivulets', 'NOUN'),\n",
       "  ('almost', 'ADV'),\n",
       "  ('first', 'ADJ'),\n",
       "  ('relying', 'VERB')],\n",
       " [('the', 'DET'),\n",
       "  ('us', 'PRON'),\n",
       "  ('lock', 'NOUN'),\n",
       "  ('an', 'DET'),\n",
       "  ('Everyone', 'PRT'),\n",
       "  ('the', 'DET'),\n",
       "  (\"''\", '.'),\n",
       "  ('fifty', 'NUM'),\n",
       "  (',', '.'),\n",
       "  ('the', 'DET')],\n",
       " [('The', 'DET'),\n",
       "  ('lyric', 'ADJ'),\n",
       "  ('Gross', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('every', 'DET'),\n",
       "  ('Af', 'NOUN'),\n",
       "  ('oats', 'NOUN'),\n",
       "  ('for', 'ADP'),\n",
       "  ('into', 'ADP'),\n",
       "  ('the', 'DET')],\n",
       " [('there', 'ADV'),\n",
       "  ('the', 'DET'),\n",
       "  ('which', 'DET'),\n",
       "  ('pre-war', 'NUM'),\n",
       "  ('voice', 'NOUN'),\n",
       "  ('contrary', 'ADJ'),\n",
       "  ('earn', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('grows', 'VERB'),\n",
       "  ('.', '.')],\n",
       " [('system', 'NOUN'),\n",
       "  ('the', 'DET'),\n",
       "  ('from', 'ADP'),\n",
       "  ('our', 'DET'),\n",
       "  ('local', 'ADJ'),\n",
       "  ('the', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('other', 'ADJ'),\n",
       "  ('the', 'DET'),\n",
       "  ('the', 'DET')],\n",
       " [('wrapped', 'VERB'),\n",
       "  ('a', 'DET'),\n",
       "  ('necessary', 'ADJ'),\n",
       "  ('the', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('everything', 'NOUN'),\n",
       "  ('his', 'DET'),\n",
       "  ('horizontal', 'ADJ'),\n",
       "  ('He', 'PRON'),\n",
       "  (',', '.')],\n",
       " [('grassfire', 'CONJ'),\n",
       "  ('.', '.'),\n",
       "  ('color', 'NOUN'),\n",
       "  ('the', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('it', 'PRON'),\n",
       "  ('decision', 'NOUN'),\n",
       "  ('to', 'PRT'),\n",
       "  ('a', 'DET'),\n",
       "  ('This', 'DET')],\n",
       " [('imagination', 'NOUN'),\n",
       "  ('these', 'DET'),\n",
       "  ('that', 'PRON'),\n",
       "  ('history', 'NOUN'),\n",
       "  ('he', 'PRON'),\n",
       "  ('its', 'DET'),\n",
       "  ('of', 'ADP'),\n",
       "  ('a', 'DET'),\n",
       "  ('which', 'DET'),\n",
       "  ('any', 'DET')],\n",
       " [('it', 'PRON'),\n",
       "  ('especially', 'ADV'),\n",
       "  ('him', 'PRON'),\n",
       "  ('I', 'PRON'),\n",
       "  ('the', 'DET'),\n",
       "  ('with', 'ADP'),\n",
       "  (',', '.'),\n",
       "  ('in', 'ADP'),\n",
       "  ('which', 'DET'),\n",
       "  ('radio', 'NOUN')],\n",
       " [('how', 'ADV'),\n",
       "  ('nevertheless', 'ADV'),\n",
       "  ('only', 'ADV'),\n",
       "  ('the', 'DET'),\n",
       "  ('Mullenax', 'CONJ'),\n",
       "  ('``', '.'),\n",
       "  ('the', 'DET'),\n",
       "  ('back', 'ADV'),\n",
       "  ('of', 'ADP'),\n",
       "  ('a', 'DET')],\n",
       " [('get', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('writing', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('but', 'CONJ'),\n",
       "  ('.', '.'),\n",
       "  ('the', 'DET'),\n",
       "  ('begun', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('.', '.')],\n",
       " [('the', 'DET'),\n",
       "  ('of', 'ADP'),\n",
       "  ('.', '.'),\n",
       "  ('the', 'DET'),\n",
       "  ('350', 'NUM'),\n",
       "  ('who', 'PRON'),\n",
       "  ('.', '.'),\n",
       "  ('escapes', 'VERB'),\n",
       "  ('his', 'DET'),\n",
       "  ('he', 'PRON')],\n",
       " [('the', 'DET'),\n",
       "  ('100', 'NUM'),\n",
       "  ('The', 'DET'),\n",
       "  ('their', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('his', 'DET'),\n",
       "  ('Rosa', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('a', 'DET'),\n",
       "  ('the', 'DET')],\n",
       " [('However', 'ADV'),\n",
       "  ('the', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('like', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('.', '.'),\n",
       "  ('the', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('poured-in-place', 'CONJ')],\n",
       " [('The', 'DET'),\n",
       "  ('associate', 'ADJ'),\n",
       "  ('most', 'ADJ'),\n",
       "  (\"''\", '.'),\n",
       "  ('level', 'NOUN'),\n",
       "  ('it', 'PRON'),\n",
       "  ('the', 'DET'),\n",
       "  ('chamber', 'NOUN'),\n",
       "  ('.', '.'),\n",
       "  ('belief', 'NOUN')],\n",
       " [('his', 'DET'),\n",
       "  ('timed', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('.', '.'),\n",
       "  ('evoke', 'X'),\n",
       "  ('the', 'DET'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('what', 'DET'),\n",
       "  ('letter', 'NOUN'),\n",
       "  ('.', '.')],\n",
       " [('a', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('a', 'DET'),\n",
       "  ('Developments', 'NUM'),\n",
       "  ('that', 'DET'),\n",
       "  ('a', 'DET'),\n",
       "  ('The', 'DET'),\n",
       "  (',', '.'),\n",
       "  ('public', 'ADJ'),\n",
       "  ('handful', 'NOUN')],\n",
       " [('7', 'NUM'),\n",
       "  ('but', 'CONJ'),\n",
       "  ('a', 'DET'),\n",
       "  ('employment', 'NOUN'),\n",
       "  ('The', 'DET'),\n",
       "  ('is', 'VERB'),\n",
       "  ('radical', 'ADJ'),\n",
       "  ('Broglio', 'DET'),\n",
       "  ('of', 'ADP'),\n",
       "  ('political', 'ADJ')],\n",
       " [('it', 'PRON'),\n",
       "  ('quick', 'ADV'),\n",
       "  ('the', 'DET'),\n",
       "  ('fatter', 'ADJ'),\n",
       "  ('clientele', 'NOUN'),\n",
       "  ('mutual', 'ADJ'),\n",
       "  ('its', 'DET'),\n",
       "  ('potential', 'ADJ'),\n",
       "  ('The', 'DET'),\n",
       "  (';', '.')],\n",
       " [('exact', 'ADJ'),\n",
       "  ('the', 'DET'),\n",
       "  ('of', 'ADP'),\n",
       "  ('exercise', 'NOUN'),\n",
       "  ('superbly', 'ADV'),\n",
       "  ('special', 'ADJ'),\n",
       "  ('piazza', 'NOUN'),\n",
       "  ('cannery', 'NOUN'),\n",
       "  ('demagnification', 'NOUN'),\n",
       "  ('my', 'DET')],\n",
       " [('National', 'ADJ'),\n",
       "  ('much', 'ADJ'),\n",
       "  ('.', '.'),\n",
       "  ('Little', 'ADJ'),\n",
       "  ('exultation', 'NOUN'),\n",
       "  ('the', 'DET'),\n",
       "  ('which', 'DET'),\n",
       "  ('anti-Newtonian', 'ADV'),\n",
       "  ('by', 'ADP'),\n",
       "  ('the', 'DET')],\n",
       " [('cost', 'NOUN'),\n",
       "  ('him', 'PRON'),\n",
       "  (',', '.'),\n",
       "  ('insufficient', 'DET'),\n",
       "  ('my', 'DET'),\n",
       "  ('there', 'PRT'),\n",
       "  ('!', '.'),\n",
       "  (',', '.'),\n",
       "  ('colorful', 'ADJ'),\n",
       "  ('first', 'ADJ')],\n",
       " [('the', 'DET'),\n",
       "  ('fast-spreading', 'ADJ'),\n",
       "  ('the', 'DET'),\n",
       "  ('fit', 'NOUN'),\n",
       "  ('her', 'DET'),\n",
       "  ('a', 'DET'),\n",
       "  ('West', 'ADJ'),\n",
       "  ('.', '.'),\n",
       "  ('this', 'DET'),\n",
       "  ('Foreign', 'ADJ')],\n",
       " [('that', 'PRON'),\n",
       "  ('social', 'ADJ'),\n",
       "  ('the', 'DET'),\n",
       "  ('them', 'PRON'),\n",
       "  ('at', 'ADP'),\n",
       "  ('leg', 'NOUN'),\n",
       "  ('The', 'DET'),\n",
       "  ('in', 'ADP'),\n",
       "  ('whose', 'DET'),\n",
       "  ('The', 'DET')],\n",
       " [('pilot', 'NOUN'),\n",
       "  ('who', 'PRON'),\n",
       "  ('waterskiing', 'CONJ'),\n",
       "  ('small', 'ADJ'),\n",
       "  ('another', 'DET'),\n",
       "  ('conflict', 'DET'),\n",
       "  ('of', 'ADP'),\n",
       "  (',', '.'),\n",
       "  ('concise', 'PRON'),\n",
       "  ('the', 'DET')],\n",
       " [('so', 'ADV'),\n",
       "  ('one', 'NUM'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('horsehair', 'DET'),\n",
       "  ('a', 'DET'),\n",
       "  ('sovereign', 'ADJ'),\n",
       "  ('of', 'ADP'),\n",
       "  ('she', 'PRON'),\n",
       "  ('an', 'DET'),\n",
       "  ('her', 'DET')],\n",
       " [('the', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  (',', '.'),\n",
       "  (',', '.'),\n",
       "  ('interlobular', 'ADJ'),\n",
       "  ('is', 'VERB'),\n",
       "  ('many', 'ADJ'),\n",
       "  ('to', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('other', 'ADJ')],\n",
       " [('one', 'NOUN'),\n",
       "  ('to', 'ADP'),\n",
       "  ('then', 'ADV'),\n",
       "  ('the', 'DET'),\n",
       "  ('this', 'DET'),\n",
       "  ('political', 'ADJ'),\n",
       "  ('the', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('or', 'CONJ'),\n",
       "  ('been', 'VERB')],\n",
       " [('nonshifters', 'NOUN'),\n",
       "  ('help', 'VERB'),\n",
       "  ('``', '.'),\n",
       "  ('the', 'DET'),\n",
       "  ('to', 'PRT'),\n",
       "  ('a', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('in', 'ADP'),\n",
       "  ('his', 'DET'),\n",
       "  ('research', 'NOUN')],\n",
       " [('these', 'DET'),\n",
       "  ('personal', 'ADJ'),\n",
       "  ('.', '.'),\n",
       "  ('There', 'PRT'),\n",
       "  ('normal', 'ADJ'),\n",
       "  ('earlier', 'ADJ'),\n",
       "  ('grandmother', 'NUM'),\n",
       "  ('was', 'VERB'),\n",
       "  ('this', 'DET'),\n",
       "  ('the', 'DET')],\n",
       " [('equally', 'ADV'),\n",
       "  ('men', 'NOUN'),\n",
       "  ('excellent', 'ADJ'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('capital', 'NOUN'),\n",
       "  ('15', 'NUM'),\n",
       "  ('team', 'NOUN'),\n",
       "  ('the', 'DET'),\n",
       "  ('who', 'PRON'),\n",
       "  ('my', 'DET')],\n",
       " [('his', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('in', 'ADP'),\n",
       "  ('he', 'PRON'),\n",
       "  ('.', '.'),\n",
       "  ('moisture', 'NOUN'),\n",
       "  ('(', '.'),\n",
       "  ('landscapes', 'NOUN'),\n",
       "  ('lives', 'NOUN'),\n",
       "  ('a', 'DET')],\n",
       " [('she', 'PRON'),\n",
       "  ('could', 'VERB'),\n",
       "  ('no', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('provisional', 'ADJ'),\n",
       "  ('especially', 'ADV'),\n",
       "  ('with', 'ADP'),\n",
       "  ('when', 'ADV'),\n",
       "  ('the', 'DET'),\n",
       "  (\"brother's\", 'NOUN')],\n",
       " [(\"children's\", 'VERB'),\n",
       "  ('latches', 'DET'),\n",
       "  ('This', 'DET'),\n",
       "  ('weeks', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('from', 'ADP'),\n",
       "  ('(', '.'),\n",
       "  ('the', 'DET'),\n",
       "  ('Cheshire', 'VERB'),\n",
       "  ('the', 'DET')],\n",
       " [('and', 'CONJ'),\n",
       "  ('wicked', 'ADJ'),\n",
       "  ('``', '.'),\n",
       "  (',', '.'),\n",
       "  ('in', 'ADP'),\n",
       "  ('another', 'DET'),\n",
       "  ('particular', 'ADJ'),\n",
       "  ('Yet', 'ADV'),\n",
       "  ('between', 'ADP'),\n",
       "  ('which', 'DET')],\n",
       " [('and', 'CONJ'),\n",
       "  ('59', 'ADJ'),\n",
       "  ('a', 'DET'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('come', 'VERB'),\n",
       "  ('a', 'DET'),\n",
       "  ('some', 'DET'),\n",
       "  ('obvious', 'ADJ'),\n",
       "  ('hoping', 'VERB'),\n",
       "  ('the', 'DET')],\n",
       " [('much', 'ADV'),\n",
       "  ('A', 'DET'),\n",
       "  ('Just', 'ADV'),\n",
       "  ('vague', 'ADJ'),\n",
       "  ('own', 'ADJ'),\n",
       "  ('an', 'DET'),\n",
       "  ('second', 'ADJ'),\n",
       "  ('supplied', 'VERB'),\n",
       "  ('poem', 'NOUN'),\n",
       "  ('the', 'DET')],\n",
       " [('He', 'PRON'),\n",
       "  ('his', 'DET'),\n",
       "  ('other', 'ADJ'),\n",
       "  ('the', 'DET'),\n",
       "  ('jewel', 'PRT'),\n",
       "  ('Leavenworth', 'NOUN'),\n",
       "  ('a', 'DET'),\n",
       "  ('using', 'VERB'),\n",
       "  ('a', 'DET'),\n",
       "  ('sure', 'ADJ')],\n",
       " [('you', 'PRON'),\n",
       "  ('are', 'VERB'),\n",
       "  ('his', 'DET'),\n",
       "  ('a', 'DET'),\n",
       "  ('her', 'DET'),\n",
       "  ('gate', 'NOUN'),\n",
       "  ('our', 'DET'),\n",
       "  ('are', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('helpful', 'ADJ')],\n",
       " [('remote', 'ADJ'),\n",
       "  ('human', 'ADJ'),\n",
       "  ('which', 'DET'),\n",
       "  ('up', 'PRT'),\n",
       "  ('yards', 'NOUN'),\n",
       "  ('song', 'NOUN'),\n",
       "  ('``', '.'),\n",
       "  ('which', 'DET'),\n",
       "  ('Strange', 'ADJ'),\n",
       "  ('``', '.')],\n",
       " [('day', 'NOUN'),\n",
       "  ('uniform', 'ADJ'),\n",
       "  ('point', 'NOUN'),\n",
       "  ('inevitable', 'ADJ'),\n",
       "  ('found', 'VERB'),\n",
       "  ('2', 'NUM'),\n",
       "  ('her', 'DET'),\n",
       "  ('its', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('we', 'PRON')],\n",
       " [('Why', 'ADV'),\n",
       "  ('seers', 'ADJ'),\n",
       "  ('sure', 'ADJ'),\n",
       "  ('what', 'DET'),\n",
       "  ('its', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('counsel', 'X'),\n",
       "  (',', '.'),\n",
       "  ('an', 'DET')],\n",
       " [('encroachment', 'PRON'),\n",
       "  ('rebuttal', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('higher', 'ADJ'),\n",
       "  ('(', '.'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('was', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('which', 'DET')],\n",
       " [('He', 'PRON'),\n",
       "  ('Thermogravimetric', 'ADJ'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('slowly', 'ADV'),\n",
       "  ('this', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('.', '.'),\n",
       "  (',', '.'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('a', 'DET')],\n",
       " [('another', 'DET'),\n",
       "  ('an', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('before', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('from', 'ADP'),\n",
       "  ('a', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('the', 'DET')],\n",
       " [('and', 'CONJ'),\n",
       "  ('to', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('.', '.'),\n",
       "  ('what', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('described', 'VERB'),\n",
       "  ('a', 'DET'),\n",
       "  ('own', 'ADJ'),\n",
       "  ('consequential', 'ADJ')],\n",
       " [('through', 'ADP'),\n",
       "  ('.', '.'),\n",
       "  (',', '.'),\n",
       "  ('the', 'DET'),\n",
       "  ('general', 'ADJ'),\n",
       "  ('that', 'DET'),\n",
       "  ('his', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('straight-haired', 'ADV'),\n",
       "  ('the', 'DET')],\n",
       " [('depending', 'ADP'),\n",
       "  ('anxious', 'ADJ'),\n",
       "  ('on', 'ADP'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('was', 'VERB'),\n",
       "  ('Bell', 'NUM'),\n",
       "  ('General', 'ADJ'),\n",
       "  ('American', 'ADJ'),\n",
       "  ('at', 'ADP'),\n",
       "  ('the', 'DET')],\n",
       " [('the', 'DET'),\n",
       "  ('.', '.'),\n",
       "  ('was', 'VERB'),\n",
       "  ('.', '.'),\n",
       "  ('no', 'ADV'),\n",
       "  ('over', 'ADP'),\n",
       "  ('appropriate', 'ADJ'),\n",
       "  ('Department', 'NOUN'),\n",
       "  ('the', 'DET'),\n",
       "  ('official', 'ADJ')],\n",
       " [('a', 'DET'),\n",
       "  ('Methodist', 'NOUN'),\n",
       "  ('which', 'DET'),\n",
       "  ('what', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('his', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('the', 'DET'),\n",
       "  ('Both', 'DET'),\n",
       "  ('to', 'PRT')],\n",
       " [('polling', 'PRT'),\n",
       "  ('Ann', 'NOUN'),\n",
       "  ('Palace', 'NOUN'),\n",
       "  (\"Danny's\", 'ADJ'),\n",
       "  ('nearly', 'ADV'),\n",
       "  ('to', 'PRT'),\n",
       "  ('projects', 'NOUN'),\n",
       "  ('treelike', 'ADJ'),\n",
       "  ('His', 'DET'),\n",
       "  ('and', 'CONJ')]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1330f9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove unknown tag X\n",
    "for i in test_data:\n",
    "    for j in i:\n",
    "        if j[1]=='X':\n",
    "            i.remove((j[0],j[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "639a3879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#break up the input data structure into lists of sentences and tags\n",
    "test_sentences = []\n",
    "for i in test_data:\n",
    "    test_sentence = []\n",
    "    for j in range(len(i)):\n",
    "        test_sentence.append(i[j][0])\n",
    "    test_sentences.append(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d9d25f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tags = []\n",
    "for i in test_data:\n",
    "    test_tag = []\n",
    "    for j in range(len(i)):\n",
    "        test_tag.append(i[j][1])\n",
    "    test_tags.append(test_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e93fe79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Synthetic data is based on nltk,\n",
    "#Some tags are named different to torchtext's\n",
    "#Fix here\n",
    "for i in test_tags:\n",
    "    for j in range(len(i)):\n",
    "        if i[j] == '.':\n",
    "            i[j] = 'PUNCT'\n",
    "        if i[j] == 'PRT':\n",
    "            i[j] = 'PART'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f9f95ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on tags using the synthetic data\n",
    "pred_tags_output=[]\n",
    "for i in range(len(test_sentences)):\n",
    "    sentence = [i for i in test_sentences[i]]\n",
    "    # Vectorize the sentence. 0 stands for unknown tokens in torchtext\n",
    "    vectorized_text = [text.vocab.stoi[t] for t in [j for j in test_sentences[i]]]\n",
    "    # Turn the vectorized sentence into a tensor object\n",
    "    tensor = torch.LongTensor(vectorized_text).unsqueeze(-1).to('cuda')\n",
    "    pred = net(tensor)\n",
    "    # Make classification for each token based on the tag with the highest probability\n",
    "    pred_tags = [tags.vocab.itos[i.item()] for i in pred.argmax(-1)]\n",
    "    pred_tags_output.append(pred_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38eec6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['residential',\n",
       "  \"''\",\n",
       "  'the',\n",
       "  'the',\n",
       "  'the',\n",
       "  'some',\n",
       "  'A',\n",
       "  'output',\n",
       "  'deeper',\n",
       "  'the'],\n",
       " ['PROPN', 'PUNCT', 'DET', 'DET', 'DET', 'DET', 'ADJ', 'NOUN', 'VERB', 'DET'],\n",
       " ['ADJ', 'PUNCT', 'DET', 'DET', 'DET', 'DET', 'DET', 'NOUN', 'ADJ', 'DET'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A brief look of how the model performs before checking the metrics\n",
    "\n",
    "test_sentences[1],pred_tags_output[1],test_tags[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2608bdc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['hold', 'E', 'its', 'a', 'deferments', '2', 'the', 'the', 'and', 'work'],\n",
       " ['NOUN', 'VERB', 'PRON', 'DET', 'NOUN', 'NUM', 'DET', 'DET', 'CCONJ', 'NOUN'],\n",
       " ['VERB', 'NOUN', 'DET', 'DET', 'DET', 'NUM', 'DET', 'DET', 'CONJ', 'NOUN'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences[7],pred_tags_output[7],test_tags[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45b66391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Added classifier report to the code so that it displays precision, recall, and f1 score\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b71d9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Accuracy: 0.666\n",
      "\t Precision: 0.658\n",
      "\t F1 Score: 0.609\n",
      "\t Recall: 0.630\n"
     ]
    }
   ],
   "source": [
    "pred_tags_output_flat = [i for j in pred_tags_output for i in j]\n",
    "#Keep consistency, torchtext divides up CONJ into CCONJ and SCONJ for instance\n",
    "#Merge them into CONJ like NLTK\n",
    "for i in range(len(pred_tags_output_flat)):\n",
    "    if pred_tags_output_flat[i] == \"CCONJ\":\n",
    "        pred_tags_output_flat[i] = \"CONJ\"\n",
    "    elif pred_tags_output_flat[i] == \"SCONJ\":\n",
    "        pred_tags_output_flat[i] = \"CONJ\"\n",
    "    elif pred_tags_output_flat[i] == \"PROPN\":\n",
    "        pred_tags_output_flat[i] = \"NOUN\"\n",
    "    elif pred_tags_output_flat[i] == \"AUX\":\n",
    "        pred_tags_output_flat[i] = \"VERB\"\n",
    "test_tags_flat = [i for j in test_tags for i in j]\n",
    "#imbalanced labels\n",
    "print(f'\\t Accuracy: {sklearn.metrics.accuracy_score(test_tags_flat, pred_tags_output_flat):.3f}')\n",
    "print(f'\\t Precision: {sklearn.metrics.precision_score(test_tags_flat, pred_tags_output_flat, average=\"macro\", zero_division = 0):.3f}')\n",
    "print(f'\\t F1 Score: {sklearn.metrics.f1_score(test_tags_flat, pred_tags_output_flat, average=\"macro\", zero_division = 0):.3f}')\n",
    "print(f'\\t Recall: {sklearn.metrics.recall_score(test_tags_flat, pred_tags_output_flat, average=\"macro\", zero_division = 0):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "708e79ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Accuracy: 0.666\n",
      "\t Precision: 0.778\n",
      "\t F1 Score: 0.691\n",
      "\t Recall: 0.666\n"
     ]
    }
   ],
   "source": [
    "#balanced labels\n",
    "print(f'\\t Accuracy: {sklearn.metrics.accuracy_score(test_tags_flat, pred_tags_output_flat):.3f}')\n",
    "print(f'\\t Precision: {sklearn.metrics.precision_score(test_tags_flat, pred_tags_output_flat, average=\"weighted\", zero_division = 0):.3f}')\n",
    "print(f'\\t F1 Score: {sklearn.metrics.f1_score(test_tags_flat, pred_tags_output_flat, average=\"weighted\", zero_division = 0):.3f}')\n",
    "print(f'\\t Recall: {sklearn.metrics.recall_score(test_tags_flat, pred_tags_output_flat, average=\"weighted\", zero_division = 0):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8369d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.81      0.53      0.64       118\n",
      "         ADP       0.84      0.77      0.80        66\n",
      "         ADV       0.64      0.54      0.58        52\n",
      "        CONJ       0.78      0.78      0.78        40\n",
      "         DET       0.98      0.65      0.78       358\n",
      "        NOUN       0.43      0.73      0.54       114\n",
      "         NUM       0.92      0.55      0.69        20\n",
      "        PART       0.25      0.05      0.09        19\n",
      "        PRON       0.27      0.72      0.39        40\n",
      "       PUNCT       1.00      0.88      0.93        80\n",
      "        VERB       0.35      0.73      0.47        51\n",
      "\n",
      "    accuracy                           0.67       958\n",
      "   macro avg       0.66      0.63      0.61       958\n",
      "weighted avg       0.78      0.67      0.69       958\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(test_tags_flat, pred_tags_output_flat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37d06ab",
   "metadata": {},
   "source": [
    "## Testing on Real World Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d84eda3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\jhlda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\jhlda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#real world data\n",
    "#Using brown from nltk library\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3e36ba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "brown = brown.tagged_sents(tagset='universal')[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6318b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in brown:\n",
    "    for j in i:\n",
    "        if j[1]=='X':\n",
    "            i.remove((j[0],j[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dc7c6bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_test_sentences = []\n",
    "for i in brown:\n",
    "    real_test_sentence = []\n",
    "    for j in range(len(i)):\n",
    "        real_test_sentence.append(i[j][0])\n",
    "    real_test_sentences.append(real_test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "138839a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_test_tags = []\n",
    "for i in brown:\n",
    "    real_test_tag = []\n",
    "    for j in range(len(i)):\n",
    "        real_test_tag.append(i[j][1])\n",
    "    real_test_tags.append(real_test_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5c7351d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in real_test_tags:\n",
    "    for j in range(len(i)):\n",
    "        if i[j] == '.':\n",
    "            i[j] = 'PUNCT'\n",
    "        if i[j] == 'PRT':\n",
    "            i[j] = 'PART'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "abe6423c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')], [('The', 'DET'), ('jury', 'NOUN'), ('further', 'ADV'), ('said', 'VERB'), ('in', 'ADP'), ('term-end', 'NOUN'), ('presentments', 'NOUN'), ('that', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('Executive', 'ADJ'), ('Committee', 'NOUN'), (',', '.'), ('which', 'DET'), ('had', 'VERB'), ('over-all', 'ADJ'), ('charge', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('election', 'NOUN'), (',', '.'), ('``', '.'), ('deserves', 'VERB'), ('the', 'DET'), ('praise', 'NOUN'), ('and', 'CONJ'), ('thanks', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('of', 'ADP'), ('Atlanta', 'NOUN'), (\"''\", '.'), ('for', 'ADP'), ('the', 'DET'), ('manner', 'NOUN'), ('in', 'ADP'), ('which', 'DET'), ('the', 'DET'), ('election', 'NOUN'), ('was', 'VERB'), ('conducted', 'VERB'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3ab6deeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_pred_tags_output=[]\n",
    "for i in range(len(real_test_sentences)):\n",
    "    sentence = [i for i in real_test_sentences[i]]\n",
    "    vectorized_text = [text.vocab.stoi[t] for t in [j for j in real_test_sentences[i]]]\n",
    "    tensor = torch.LongTensor(vectorized_text).unsqueeze(-1).to('cuda')\n",
    "    pred = net(tensor)\n",
    "    pred_tags = [tags.vocab.itos[i.item()] for i in pred.argmax(-1)]\n",
    "    real_pred_tags_output.append(pred_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "99b7cf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Accuracy: 0.855\n",
      "\t Precision: 0.749\n",
      "\t F1 Score: 0.728\n",
      "\t Recall: 0.732\n"
     ]
    }
   ],
   "source": [
    "real_pred_tags_output_flat = [i for j in real_pred_tags_output for i in j]\n",
    "real_test_tags_flat = [i for j in real_test_tags for i in j]\n",
    "for i in range(len(real_pred_tags_output_flat)):\n",
    "    if real_pred_tags_output_flat[i] == \"CCONJ\":\n",
    "        real_pred_tags_output_flat[i] = \"CONJ\"\n",
    "    elif real_pred_tags_output_flat[i] == \"SCONJ\":\n",
    "        real_pred_tags_output_flat[i] = \"CONJ\"\n",
    "    elif real_pred_tags_output_flat[i] == \"PROPN\":\n",
    "        real_pred_tags_output_flat[i] = \"NOUN\"\n",
    "    elif real_pred_tags_output_flat[i] == \"AUX\":\n",
    "        real_pred_tags_output_flat[i] = \"VERB\"\n",
    "#imbalanced labels\n",
    "print(f'\\t Accuracy: {sklearn.metrics.accuracy_score(real_test_tags_flat, real_pred_tags_output_flat):.3f}')\n",
    "print(f'\\t Precision: {sklearn.metrics.precision_score(real_test_tags_flat, real_pred_tags_output_flat, average=\"macro\", zero_division = 0):.3f}')\n",
    "print(f'\\t F1 Score: {sklearn.metrics.f1_score(real_test_tags_flat, real_pred_tags_output_flat, average=\"macro\", zero_division = 0):.3f}')\n",
    "print(f'\\t Recall: {sklearn.metrics.recall_score(real_test_tags_flat, real_pred_tags_output_flat, average=\"macro\", zero_division = 0):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dd106a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Accuracy: 0.855\n",
      "\t Precision: 0.875\n",
      "\t F1 Score: 0.859\n",
      "\t Recall: 0.855\n"
     ]
    }
   ],
   "source": [
    "#balanced labels\n",
    "print(f'\\t Accuracy: {sklearn.metrics.accuracy_score(real_test_tags_flat, real_pred_tags_output_flat):.3f}')\n",
    "print(f'\\t Precision: {sklearn.metrics.precision_score(real_test_tags_flat, real_pred_tags_output_flat, average=\"weighted\", zero_division = 0):.3f}')\n",
    "print(f'\\t F1 Score: {sklearn.metrics.f1_score(real_test_tags_flat, real_pred_tags_output_flat, average=\"weighted\", zero_division = 0):.3f}')\n",
    "print(f'\\t Recall: {sklearn.metrics.recall_score(real_test_tags_flat, real_pred_tags_output_flat, average=\"weighted\", zero_division = 0):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "de2c3be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.65      0.81      0.72       120\n",
      "         ADP       0.96      0.80      0.87       256\n",
      "         ADV       0.91      0.67      0.77        58\n",
      "        CONJ       0.60      0.98      0.74        47\n",
      "         DET       0.96      0.76      0.85       279\n",
      "        NOUN       0.84      0.92      0.88       721\n",
      "         NUM       0.86      0.62      0.72        39\n",
      "        PART       0.85      0.70      0.77        57\n",
      "        PRON       0.47      0.75      0.58        48\n",
      "       PUNCT       1.00      0.86      0.93       259\n",
      "        VERB       0.89      0.92      0.91       384\n",
      "           X       0.00      1.00      0.00         0\n",
      "\n",
      "    accuracy                           0.85      2268\n",
      "   macro avg       0.75      0.82      0.73      2268\n",
      "weighted avg       0.88      0.85      0.86      2268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(real_test_tags_flat, real_pred_tags_output_flat, zero_division = 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
